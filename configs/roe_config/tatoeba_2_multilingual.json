{
    "input_length" : 128,
    "output_length" : 128,
    "dataset_length" : 10000,
    "multilingual": true,
    "num_train_epochs" : 20,
    "output_dir" : "../outputs/tatoeba_2.pt",
    "dataset" : ["tatoeba"],
    "valid_dataset" :["tatoeba"],
    "dataset_version" : "full",
    "train_batch_size" : 4,
    "eval_batch_size" : 16,    
    "learning_rate" : 1e-4,
    "model" : "google/mt5-xl",
    "gradient_accumulation_steps" : 20,
    "ngpu" : 16,
    "num_workers" : 0,
    "wandb_log": true,
    "wandb_project": "RoE ICML direct",
    "wandb_run_name" : "tatoeba_2",
    "mode" : "zerotune",
    "use_lr_scheduling" : false, 
    "eval_with_prompt": false,
    "fp16" : false,
    "accelerator": "ddp_sharded",
    "required_classification": false,
    "eval_with_prob": false,
    "channel": false,
    "ul_loss" : false,
    "prompt_name": "translate from kor to eng"
}