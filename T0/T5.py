
import pytorch_lightning as pl
from transformers import AutoTokenizer, T5Tokenizer, AutoModelForSeq2SeqLM, T5ForConditionalGeneration, Adafactor, MT5Config, T5Config
from pytorch_lightning.trainer.supporters import CombinedLoader
from torch.utils.data import RandomSampler
from torch.utils.data import DataLoader, ConcatDataset
from Datasets_end2end import Pretrain
from utils import ids_to_clean_text, _rougel_score
import torch
from torch.optim import AdamW
import os
import functools


def rsetattr(obj, attr, val):
    pre, _, post = attr.rpartition('.')
    return setattr(rgetattr(obj, pre) if pre else obj, post, val)

def rgetattr(obj, attr, *args):
    def _getattr(obj, attr):
        return getattr(obj, attr, *args)
    return functools.reduce(_getattr, [obj] + attr.split('.'))


class T5_small(pl.LightningModule):
    def __init__(self, args):
        super(T5_small, self).__init__()
        self.args = args
        self.tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path) if args.multilingual else T5Tokenizer.from_pretrained(args.model_name_or_path)
        self.val_count = 0
        self.epoch = 0
        self.model =AutoModelForSeq2SeqLM.from_pretrained(args.model_name_or_path) if args.multilingual else T5ForConditionalGeneration.from_pretrained(args.model_name_or_path)
        if args.random == True:
            config = MT5Config.from_pretrained(args.model_name_or_path) if args.multilingual else T5Config.from_pretrained(args.model_name_or_path)
            self.model = AutoModelForSeq2SeqLM(config) if args.multilingual else T5ForConditionalGeneration(config)

    def get_dataset(self, dataset, tokenizer, type_path, args):
        dataset = Pretrain(dataset=dataset, tokenizer=tokenizer, type_path=type_path, input_length=args.max_input_length, 
                                output_length=args.max_output_length, args=args)
        if type_path == 'validation':
            self.ids_to_answers = dataset.ids_to_answers
        return dataset
        
    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None):
        return self.model(
            input_ids,
            attention_mask=attention_mask,
            decoder_input_ids=decoder_input_ids,
            decoder_attention_mask=decoder_attention_mask,
            labels=lm_labels,
        )
    
    def _step(self, batch):
        if not self.args.channel:
            if not self.args.ul_loss:
                lm_labels = batch["target_ids"]
                lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100
                outputs = self(
                    input_ids= batch["source_ids"],
                    attention_mask= batch["source_mask"],
                    lm_labels=lm_labels,
                    decoder_attention_mask= batch["target_mask"]
                )
                loss = outputs[0]
            else: 
                lm_labels = batch["target_ids"]
                lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100
                outputs = self(
                    input_ids = torch.cat([batch["source_ids"], batch["source_ids"]]),
                    attention_mask = torch.cat([batch["source_mask"], batch["source_mask"]]),
                    lm_labels = torch.cat([lm_labels[:,:self.args.max_output_length], lm_labels[:,self.args.max_output_length:]]),
                    decoder_attention_mask= torch.cat([batch["target_mask"][:,:self.args.max_output_length],batch["target_mask"][:,self.args.max_output_length:]])
                )
                # print("outputs shape",outputs.logits.shape)
                lm_logits = outputs.logits[:batch["source_ids"].size(0),:,:]
                un_logits = outputs.logits[batch["source_ids"].size(0):,:,:]

                lm_loss = torch.nn.functional.cross_entropy(lm_logits.view(-1, lm_logits.size(-1)), lm_labels[:,:self.args.max_output_length].view(-1))
                cand_loglikely = -torch.nn.functional.cross_entropy(
                    un_logits.flatten(0, 1), lm_labels[:,self.args.max_output_length:].flatten(0, 1), reduction="none"
                ).view(batch["source_ids"].size(0), 2, -1)

                cand_loglikely += (lm_labels[:,self.args.max_output_length:] < 0).view(batch["source_ids"].size(0), 2, -1) * -100
                unlikely_loss = -torch.log(1 - torch.exp(cand_loglikely) + 1e-2).sum() / (cand_loglikely != -100).sum()
                # print(unlikely_loss)
                loss = lm_loss + self.args.ul_weight * unlikely_loss

                self.log("train_lm_loss", lm_loss)
                self.log("train_ul_loss", unlikely_loss)
        else:    
            if not self.args.ul_loss:
                lm_labels = batch["target_ids"]
                lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100
                outputs = self(
                    input_ids= batch["source_ids"],
                    attention_mask= batch["source_mask"],
                    lm_labels=lm_labels,
                    decoder_attention_mask= batch["target_mask"]
                )
                loss = outputs[0]
            else:
                lm_labels = batch["target_ids"]
                lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100
                outputs = self(
                    input_ids = torch.cat([batch["source_ids"][:,:self.args.max_input_length], batch["source_ids"][:,self.args.max_input_length:]]),
                    attention_mask = torch.cat([batch["source_mask"][:,:self.args.max_input_length], batch["source_mask"][:,self.args.max_input_length:]]),
                    lm_labels = torch.cat([lm_labels, lm_labels]),
                    decoder_attention_mask= torch.cat([batch["target_mask"],batch["target_mask"]])
                )
                # print("outputs shape",outputs.logits.shape)
                lm_logits = outputs.logits[:batch["source_ids"].size(0),:,:]
                un_logits = outputs.logits[batch["source_ids"].size(0):,:,:]
                lm_loss = torch.nn.functional.cross_entropy(lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))
                cand_loglikely = -torch.nn.functional.cross_entropy(
                    un_logits.flatten(0, 1), lm_labels.flatten(0, 1), reduction="none"
                ).view(batch["source_ids"].size(0), 2, -1)
                cand_loglikely += (lm_labels < 0).view(batch["source_ids"].size(0), 2, -1) * -100
                unlikely_loss = -torch.log(1 - torch.exp(cand_loglikely) + 1e-2).sum() / (cand_loglikely != -100).sum()
                # print(unlikely_loss)
                loss = lm_loss + self.args.ul_weight * unlikely_loss

                self.log("train_lm_loss", lm_loss)
                self.log("train_ul_loss", unlikely_loss)
        return loss

    def _generative_step(self, batch, batch_idx):  
        keys = batch["data_label"][0]
        output_label = batch["label"].tolist()
        if self.args.channel:
            prob_list = []
            with torch.no_grad():
                for index in range(int(batch["source_ids"].shape[1])):#option에 대해서
                    
                    lm_channel_labels = batch["target_ids"][:,index,:]
                    lm_channel_labels[lm_channel_labels[:, :] == self.tokenizer.pad_token_id] = -100
                    output_label = batch["label"].tolist()
                    #print(lm_channel_labels.shape)
                    output_channel = self.model(
                        input_ids=batch["source_ids"][:,index,:].contiguous().cuda(),
                        attention_mask=batch["source_mask"][:,index,:].contiguous().cuda(),
                        labels=lm_channel_labels.contiguous().cuda(),
                        decoder_attention_mask=batch["target_mask"][:,index,:].contiguous().cuda()
                    )

                    logits_channel = batch["target_mask"][:,index,:].cuda().unsqueeze(-1) * torch.log_softmax(output_channel.logits, dim=-1)

                    lm_channel_labels=lm_channel_labels.cuda().unsqueeze(-1)
                    seq_token_log_prob=torch.zeros(lm_channel_labels.shape)
                    #print(seq_token_log_prob.shape, logits.shape, lm_labels.shape)
                    for i in range(lm_channel_labels.shape[0]):
                        for j in range(lm_channel_labels.shape[1]):
                            seq_token_log_prob[i][j][0] = logits_channel[i][j][lm_channel_labels[i][j][0]]
                    seq_channel_log_prob = seq_token_log_prob.squeeze(dim=-1).sum(dim=-1)
                    #print("seq_channel",seq_channel_log_prob)
                    prob_list.append(seq_channel_log_prob.unsqueeze(-1))

                concat = torch.cat(prob_list, 1).view(-1,int(batch["source_ids"].shape[1]))
                # print(concat.shape, concat)
                predictions = concat.argmax(dim=1)
            acc_score = sum(list(map(lambda v: v[0] ==v[1],zip(predictions,output_label))))    
                # if self.args.mode == 'zerotune':
                #self.log(f'acc_score_{keys}', acc_score/int(batch["source_ids"].shape[1]), prog_bar=True, logger=True)
            return acc_score, int(batch["source_ids"].shape[0])
        else:
            accuracy_correct_num=0
            rouge_score=0
            total_cnt = 0
            if self.args.eval_with_prob:
                prob_list = []
                with torch.no_grad():
                    for index in range(len(batch["option_list"])):
                        option = batch["option_list"]
                        option_ = self.tokenizer.batch_encode_plus(option[index], max_length=self.args.max_output_length,
                                                        padding=True, truncation=True, return_tensors="pt")
                        lm_labels = option_["input_ids"].expand(len(batch['source_ids']), -1)
                        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100
                        outputs = self.model(
                            input_ids=batch["source_ids"].cuda(),
                            attention_mask=batch["source_mask"].cuda(),
                            labels=lm_labels.cuda(),
                            decoder_attention_mask=option_["attention_mask"].cuda()
                        )
                        #print("outputs", outputs[0])
                        logits = option_["attention_mask"].cuda().unsqueeze(-1) * torch.log_softmax(outputs.logits, dim=-1)
                        lm_labels=lm_labels.cuda().unsqueeze(-1)
                        seq_token_log_prob=torch.zeros(lm_labels.shape)
                        #print(seq_token_log_prob.shape, logits.shape, lm_labels.shape)
                        for i in range(lm_labels.shape[0]):
                            for j in range(lm_labels.shape[1]):
                                seq_token_log_prob[i][j][0] = logits[i][j][lm_labels[i][j][0]]
                        seq_log_prob = seq_token_log_prob.squeeze(dim=-1).sum(dim=-1)
                        prob_list.append(seq_log_prob)
                    concat = torch.cat(prob_list).view(-1,len(batch['source_ids']))
                    #print(concat)
                    predictions = concat.argmax(dim=0)
                    dec = [batch["option_list"][i.item()][elem_num] for elem_num, i in enumerate(predictions)]
            else:
                outs = self.model.generate(
                    batch["source_ids"].cuda(),
                    attention_mask=batch["source_mask"].cuda(),
                    use_cache=True,
                    decoder_attention_mask=batch['target_mask'].cuda(),
                    max_length=self.args.max_output_length,
                    num_beams=2,
                    early_stopping=True,
                )
                dec = ids_to_clean_text(self.tokenizer, outs)
            texts = [self.tokenizer.decode(ids) for ids in batch['source_ids']]
            targets = ids_to_clean_text(self.tokenizer, batch['target_ids']) 
            if self.args.required_classification == False and self.args.eval_with_prob == False:    
                for i in range(len(batch['source_ids'])):
                    total_cnt+=1
                    ground_truth = targets[i]
                    predicted = dec[i]
                    print("input:", texts[i])
                    print("prediction:",predicted)
                
                    print("ground_truth", ground_truth)
                    rouge = _rougel_score(predicted, ground_truth)
                    rouge_score += rouge
                    print("rouge score is", rouge)
                    acc_score = rouge_score
            else:
                total_cnt+=len(batch['source_ids'])
                acc_score = sum(list(map(lambda v: v[0] ==v[1],zip(predictions,output_label)))) 
                    # accuracy = accuracy_match_score_normalize(predicted, ground_truth)
                    # if accuracy == 1:
                    #     accuracy_correct_num+=1

                # print(len(batch['source_ids']))
                    
            return acc_score, total_cnt


    def configure_optimizers(self):
        model = self.model
        no_decay = ["bias", "LayerNorm.weight"]
        optimizer_grouped_parameters = [
            {
                "params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
                "weight_decay": self.args.weight_decay,
            },
            {
                "params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
                "weight_decay": 0.0,
            },
        ]
        if self.args.method != 'residual':
            optimizer = Adafactor(optimizer_grouped_parameters, lr=self.args.learning_rate, scale_parameter=False, relative_step=False)
        else: 
            optimizer = AdamW(self.model.parameters(), lr=self.args.learning_rate)

        if self.args.use_lr_scheduling:
            len_data = len(self.train_dataloader())
            denomniator = (self.args.n_gpu * self.args.gradient_accumulation_steps)
            steps_per_epoch = ( len_data // denomniator ) + 1
            lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=self.args.learning_rate, steps_per_epoch=steps_per_epoch, pct_start=0.1, epochs=self.args.num_train_epochs, anneal_strategy='linear', cycle_momentum=False)
            return [optimizer], [{"scheduler": lr_scheduler, "interval": "step", "name": "learning rate"}]
        else:
            return [optimizer]

    def training_step(self, batch, batch_idx):
        loss = self._step(batch)
        self.log("train_loss", loss)
        return loss
    
    def on_train_epoch_end(self):
        param_dict = {}
        for name, param in self.model.named_parameters():
            param_dict[name]=param.clone().detach().cpu()
        torch.save(param_dict, self.args.output_dir[:-3]+'_last.pt') 
        print("save")


    def validation_step(self, batch, batch_idx, dataloader_idx=0):
        accs_dict = {}
        accuracy, cnt = self._generative_step(batch, batch_idx)
        return {batch['data_label'][0]: [accuracy, cnt]}
    def test_step(self, batch, batch_idx):
        for keys in batch.keys():
            self._generative_step(batch, keys, batch_idx)



    def validation_epoch_end(self, validation_step_outputs):
        
        score_dict = {}
        score = 0
        # print(self.score)
        validation_step_outputs_gather = self.all_gather(validation_step_outputs)
        # print("valid gather",validation_step_outputs_gather)
        if type(self.args.valid_dataset)==list and len(self.args.valid_dataset)>1:
            for datas in validation_step_outputs_gather:
                for output in datas:
                    for key, [accs, cnt] in output.items():
                        if key not in score_dict.keys():
                            score_dict[key]=[accs, cnt]
                        else:
                            old_acc, old_cnt = score_dict[key]
                            score_dict[key] = [old_acc + accs, old_cnt + cnt]
        else:
            for output in validation_step_outputs_gather:
                for key, [accs, cnt] in output.items():
                    if key not in score_dict.keys():
                        score_dict[key]=[accs, cnt]
                    else:
                        old_acc, old_cnt = score_dict[key]
                        score_dict[key] = [old_acc + accs, old_cnt + cnt]
        # print(score_dict)
        for key, [accs, cnt] in score_dict.items():
            self.log(f'acc_score_{key}', accs/cnt, prog_bar=True, logger=True)
            if not ('super_glue' in key or 'anli' in key or 'story_cloze' in key or 'hellaswag' in key or 'winogrande' in key):
                score += accs/cnt

        score = torch.mean(score)
        
        # print("score is ", score)
        # for output in validation_step_outputs_gather:
        #     score += torch.mean(output['score'])
        self.agg_score = score
        print("agg_score is", self.agg_score, self.val_count)
        self.log(f'acc_score_mean', self.agg_score, prog_bar=True, logger=True)
        print("enter")
        self.val_count += 1

    def train_dataloader(self):
        # zerotune - t-zero replication setting
        if self.args.mode == 'zerotune':
            total_dataset = []
            for dataset in self.args.dataset:
                dataset_elem = self.get_dataset(dataset=dataset, tokenizer=self.tokenizer, type_path="train", args=self.args)
                total_dataset.append(dataset_elem)
            train_dataset = ConcatDataset(total_dataset)
        else:
            train_dataset = self.get_dataset(dataset=self.args.dataset, tokenizer=self.tokenizer, type_path="train", args=self.args)
        sampler = RandomSampler(train_dataset)
        dataloader = DataLoader(train_dataset, sampler=sampler,  batch_size=self.args.train_batch_size, drop_last=False, num_workers=self.args.num_workers)
        return dataloader

    def val_dataloader(self):

        dataloader_dict = []
        # zerotune - t-zero replication setting
        if type(self.args.valid_dataset)==list:
            total_dataset = []
            for dataset in self.args.valid_dataset:
            # for dataset in ["cos_e/v1.11","quoref", "kilt_tasks/hotpotqa", "imdb", "ag_news", "common_gen", "xsum", "glue/qqp"]:
                if dataset=='app_reviews':
                    continue
                dataset_elem = self.get_dataset(dataset=dataset, tokenizer=self.tokenizer, type_path="validation", args=self.args)
                #print("hi", dataset, len(dataset_elem))
                dataloader_dict.append(DataLoader(dataset_elem, batch_size=self.args.eval_batch_size, drop_last=False, num_workers=self.args.num_workers))
            # train_dataset = ConcatDataset(total_dataset)
        else:
            dataset=self.args.valid_dataset
            dataset_elem = self.get_dataset(dataset=dataset, tokenizer=self.tokenizer, type_path="validation", args=self.args)
            
            dataloader_dict.append(DataLoader(dataset_elem, batch_size=self.args.eval_batch_size, drop_last=False, num_workers=self.args.num_workers))
            #print("hi", dataset, len(dataloader_dict))
        #dataloader = DataLoader(train_dataset, sampler=sampler, collate_fn=self.make_batch,  batch_size=self.args.train_batch_size, drop_last=True, num_workers=self.args.num_workers)
        combined_loaders = dataloader_dict
        return combined_loaders   

    def test_dataloader(self):
        return self.val_dataloader()