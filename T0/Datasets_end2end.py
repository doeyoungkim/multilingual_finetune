from ast import Assert
from dis import Instruction
from torch.utils.data import Dataset
import random
import torch
import json
from datasets import load_dataset
from promptsource.promptsource.templates import DatasetTemplates, choice
from utils import exact_match_score
import os
import re
import copy
import nltk
from nltk import tokenize


def bigbench_load_example(example, instruction, classification = True):
    input = example["input"]
    if not classification:
        target = example["target"]
        options = None
        label = None
    else:
        options = list(example["target_scores"].keys())
        label = list(example["target_scores"].values()).index(1)
        target = options[label]
    return instruction, input, target, options, label
def label_(name, options, answer):
    if name == "Negation template for positive and negative":
        label = options.index(answer.replace(" review.", ""))
    else:
        if options!=None:
            label = options.index(answer)
        else:
            label = -1
    return label
def multiple_replace(dictionary, text):
    # Create a regular expression  from the dictionaryary keys

    regex = re.compile("(%s)" % "|".join(map(re.escape, dictionary.keys())))

    # For each match, look-up corresponding value in dictionaryary
    String = lambda mo: dictionary[mo.string[mo.start():mo.end()]]
    return regex.sub(String , text)
def var(variation,option, target, instruct):
    variation = variation.split("/")
    option_map = variation[1].split("_")
    sentences = list(sorted(instruct.items(), key = lambda item: int(item[0].replace("<extra_id_", "").replace(">", ""))))
    rep = {}
    for index in range(len(option)):
        rep[option[index].capitalize()]= option_map[index].capitalize()
        rep[option[index].lower()]= option_map[index].lower()
    # tmp = {"Yes": "", "No": "", }
    new_option=copy.deepcopy(option)
    new_target = target
    extra_id, old_sentence = sentences[-1]
    old_sentence = multiple_replace(rep, old_sentence)
    for index in range(len(option)):
        new_option[index] = multiple_replace(rep, new_option[index]) 
    new_target = multiple_replace(rep, new_target)
    new_instruct = copy.deepcopy(instruct)
    new_instruct[extra_id]=old_sentence
    return new_option, new_target, new_instruct
def var_direct(variation,option, target, input):
    variation = variation.split("/")
    option_map = variation[1].split("_")
    
    sentences = nltk.tokenize.sent_tokenize(input)
    rep = {}
    for index in range(len(option)):
        rep[option[index].capitalize()]= option_map[index].capitalize()
        rep[option[index].lower()]= option_map[index].lower()
    # tmp = {"Yes": "", "No": "", }
    new_option=copy.deepcopy(option)
    new_target = target
    #print(rep, sentences)
    old_sentence = sentences[-1]
    old_sentence = multiple_replace(rep, old_sentence)
    for index in range(len(option)):
        new_option[index] = multiple_replace(rep, new_option[index]) 
    new_target = multiple_replace(rep, new_target)

    # if negation, label = 1-label
    new_input = ''.join(sentences[:-1])
    new_input+=old_sentence
    return new_option, new_target, new_input
def prompt_choice(whole_dataset, type_path, prompt, example_batch, args, do_random=False):
    if type_path in ['validation', 'test']:
        if args.prompt_name != None:
            ###channel validation, given choice
            return args.prompt_name
        elif do_random:
            if args.required_classification==False:
                prompt_list = list(vars(prompt)['name_to_id_mapping'].keys())
            else:
                prompt_list = list(filter(lambda x: ('Accuracy' in prompt[x].metadata.metrics) ,list(vars(prompt)['name_to_id_mapping'].keys())))
                prompt_list = list(filter(lambda x: prompt[x].get_answer_choices_list(example_batch)!=None, prompt_list))
                if whole_dataset=="hellaswag" and args.channel:
                    prompt_list = list(filter(lambda x: len(prompt[x].get_answer_choices_list(example_batch))!=2, prompt_list))
            if "copa" in whole_dataset:
                prompt_list.remove('…which may be caused by')
                prompt_list.remove('…What could happen next, C1 or C2?')
                prompt_list.remove('…why? C1 or C2')
                prompt_list.remove('…As a result, C1 or C2?')
                
            return random.choice(prompt_list)
        else:
            ###channel validation, fixed choice(not given)
            if whole_dataset == 'hellaswag':
                name = 'Randomized prompts template'
            elif 'anli' in whole_dataset:
                name = 'can we infer'
            elif 'story_cloze' in whole_dataset:
                name = 'Answer Given options'
            elif 'rte' in whole_dataset:
                name = 'does it follow that'
            elif 'copa' in whole_dataset:
                name = 'exercise'
            elif 'wic' in whole_dataset:
                name = 'question-context-meaning-with-label'
            elif 'winogrande' in whole_dataset:
                name = 'stand for'
            elif 'cos_e' in whole_dataset:
                name = 'question_description_option_id'
            elif 'paws' in whole_dataset:
                name = 'task_description-no-label'
            elif 'cb' in whole_dataset:
                name = 'can we infer'
            elif 'wsc' in whole_dataset:
                name = 'does the pronoun refer to'
            elif 'imdb' in whole_dataset:
                name = 'Movie Expressed Sentiment 2'
            elif 'ag_news' in whole_dataset:
                name = 'classify_question_first'
            else:
                name = random.choice(prompt_list)
            return name

    else:
        if args.prompt_name != None:
            return args.prompt_name
        else:
            if args.required_classification==False:
                prompt_list = list(vars(prompt)['name_to_id_mapping'].keys())
                if "trec" in whole_dataset:
                    if example_batch['coarse_label']==0:
                        prompt_list = ['fine_grained_open_context_first','fine_grained_open','what_category_best_describe','pick_the_best_descriptor','which_category_best_describes','trec1','trec2','fine_grained_DESC','fine_grained_DESC_context_first']
                    elif example_batch['coarse_label']==1:
                        prompt_list = ['fine_grained_open_context_first','fine_grained_open','what_category_best_describe','pick_the_best_descriptor','which_category_best_describes','trec1','trec2','fine_grained_ENTY']
                    elif example_batch['coarse_label']==2:
                        prompt_list = ['fine_grained_open_context_first','fine_grained_open','what_category_best_describe','pick_the_best_descriptor','which_category_best_describes','trec1','trec2','fine_grained_ABBR','fine_grained_ABBR_context_first']
                    elif example_batch['coarse_label']==3:
                        prompt_list = ['fine_grained_open_context_first','fine_grained_open','what_category_best_describe','pick_the_best_descriptor','which_category_best_describes','trec1','trec2','fine_grained_HUM','fine_grained_HUM_context_first']
                    elif example_batch['coarse_label']==4:
                        prompt_list = ['fine_grained_open_context_first','fine_grained_open','what_category_best_describe','pick_the_best_descriptor','which_category_best_describes','trec1','trec2','fine_grained_NUM_context_first','fine_grained_NUM']
                    elif example_batch['coarse_label']==5:
                        prompt_list = ['fine_grained_open_context_first','fine_grained_open','what_category_best_describe','pick_the_best_descriptor','which_category_best_describes','trec1','trec2','fine_grained_LOC','fine_grained_LOC_context_first']
                elif 'paws' in whole_dataset:
                    if example_batch['label']==0:
                        prompt_list.remove('paraphrase-task')
                elif 'wiki_qa' in whole_dataset:
                    if example_batch['label']==0:
                        prompt_list.remove('Jeopardy style')
                        prompt_list.remove('Topic Prediction - Question and Answer Pair')
                        prompt_list.remove('Generate Question from Topic')
                        prompt_list.remove('Topic Prediction - Question Only')
                        prompt_list.remove('Topic Prediction - Answer Only')
                        prompt_list.remove('Direct Answer to Question')
            else:
                prompt_list = list(filter(lambda x: ('Accuracy' in prompt[x].metadata.metrics) ,list(vars(prompt)['name_to_id_mapping'].keys())))
                prompt_list = list(filter(lambda x: prompt[x].get_answer_choices_list(example_batch)!=None, prompt_list))
                # prompt_list = list(vars(prompt)['name_to_id_mapping'].keys())
            return random.choice(prompt_list)



def dataset_prompt_setting(type_path, dataset_name, dataset_config_name, args):
    if type_path == 'bigbench':
        if dataset_config_name==None:
            path = os.path.join(args.bigbench_path, dataset_name, "task.json")
        else:
            path = os.path.join(args.bigbench_path, dataset_name, dataset_config_name, "task.json")
        with open(path, 'r') as file:
            data = json.load(file)
        instruction = data["description"]
        example = data["examples"]
        task_prefix = data.get("task_prefix", "")
        input_prefix = data.get("example_input_prefix", "\nQ: ")
        output_prefix = data.get("example_output_prefix", "\nA: ")
        choice_prefix = data.get("choice_prefix", "\n choice: ")
        append_choices_to_input = data.get("append_choices_to_input", True)

        return example, instruction, [task_prefix, input_prefix, output_prefix, choice_prefix, append_choices_to_input]
    else:
        if type_path == 'train':
            if 'tatoeba' in dataset_name:
                if dataset_name == 'tatoeba':
                    unshuffled = load_dataset("json", data_files='../data/release/v2021-08-07/eng-kor/tatoeba-kor.json', field="data")["train"]
                elif dataset_name == 'tatoeba_spa':
                    unshuffled = load_dataset("json", data_files='../data/release/v2021-08-07/eng-spa/tatoeba_spa.json', field="data")["train"]
                elif dataset_name == 'tatoeba_chi':
                    unshuffled = load_dataset("json", data_files='../data/release/v2021-08-07/eng-zho/tatoeba_zho.json', field="data")["train"]
                elif dataset_name == 'tatoeba_fra':
                    unshuffled = load_dataset("json", data_files='../data/release/v2021-08-07/eng-fra/tatoeba_fra.json', field="data")["train"]
                elif dataset_name == 'tatoeba_jap':
                    unshuffled = load_dataset("json", data_files='../data/release/v2021-08-07/eng-jpn/tatoeba_jpn.json', field="data")["train"]
                if len(unshuffled)>args.dataset_length:
                    shuffled = unshuffled.shuffle(seed=42)
                    unshuffled=shuffled.select(range(args.dataset_length))
                dataset=unshuffled
                
                prompt = DatasetTemplates('tatoeba')
            else:
                unshuffled = load_dataset(dataset_name, dataset_config_name, ignore_verifications= True)[type_path]            
                if len(unshuffled)>args.dataset_length:
                    shuffled = unshuffled.shuffle(seed=42)
                    unshuffled=shuffled.select(range(args.dataset_length))
                dataset=unshuffled
                print("###############################")
                print(dataset[0])
                print("###############################")
                prompt = DatasetTemplates(
                    f"{dataset_name}"
                    if dataset_config_name is None
                    else f"{dataset_name}/{dataset_config_name}"
                )
            return dataset, prompt, ["","","","",""]
        else: 
            if dataset_name == 'imdb' or dataset_name == 'ag_news' or dataset_name == 'amazon_polarity' or dataset_name == 'yelp_review_full' or dataset_name == 'dbpedia_14' or dataset_name == 'trec' or dataset_name == 'dream':
                type_path = 'test'
                
                unshuffled = load_dataset(dataset_name, dataset_config_name, ignore_verifications= True)[type_path]  
                prompt = DatasetTemplates(
                    f"{dataset_name}"
                    if dataset_config_name is None
                    else f"{dataset_name}/{dataset_config_name}"
                )          
            elif dataset_name == 'wiki_bio':
                type_path = 'val'
                unshuffled = load_dataset(dataset_name, dataset_config_name, ignore_verifications= True)[type_path]  
                prompt = DatasetTemplates(
                    f"{dataset_name}"
                    if dataset_config_name is None
                    else f"{dataset_name}/{dataset_config_name}"
                )                    
            elif dataset_name == 'anli':
                unshuffled = load_dataset(dataset_name, split=dataset_config_name)
                prompt = DatasetTemplates('anli')
            elif dataset_name == 'story_cloze':
                unshuffled = load_dataset("story_cloze","2016", data_dir='../data/release/v2021-08-07')[type_path]
                prompt = DatasetTemplates('story_cloze', '2016')
            elif dataset_name == 'tatoeba':
                unshuffled = load_dataset("json", data_files='../data/release/v2021-08-07/eng-kor/tatoeba_val_kor.json', field="data")["train"]
                prompt = DatasetTemplates('tatoeba')
            elif dataset_name == 'tatoeba_spa':
                unshuffled = load_dataset("json", data_files='../data/release/v2021-08-07/eng-spa/tatoeba_val_spa.json', field="data")["train"]
                prompt = DatasetTemplates('tatoeba')
            elif dataset_name == 'tatoeba_chi':
                unshuffled = load_dataset("json", data_files='../data/release/v2021-08-07/eng-zho/tatoeba_val_zho.json', field="data")["train"]
                prompt = DatasetTemplates('tatoeba')
            elif dataset_name == 'tatoeba_jap':
                unshuffled = load_dataset("json", data_files='../data/release/v2021-08-07/eng-jpn/tatoeba_val_jpn.json', field="data")["train"]
                prompt = DatasetTemplates('tatoeba')
            elif dataset_name == 'tatoeba_fra':
                unshuffled = load_dataset("json", data_files='../data/release/v2021-08-07/eng-fra/tatoeba_val_fra.json', field="data")["train"]
                prompt = DatasetTemplates('tatoeba')
            elif dataset_name == 'super_glue' or dataset_name == 'hellaswag' or dataset_name == 'winogrande' or dataset_name == 'lambada':
                unshuffled = load_dataset(dataset_name, dataset_config_name)[type_path]
                prompt = DatasetTemplates(
                    f"{dataset_name}"
                    if dataset_config_name is None
                    else f"{dataset_name}/{dataset_config_name}"
                )             
            elif (dataset_name == 'wiki_qa' and (args.prompt_name == "Jeopardy style" or args.prompt_name == "Topic Prediction - Question and Answer Pair" or args.prompt_name == "Generate Question from Topic" or args.prompt_name == "Topic Prediction - Question Only" or args.prompt_name == "Topic Prediction - Answer Only" or args.prompt_name == "Direct Answer to Question")) or (dataset_config_name == 'mrpc' and (args.prompt_name == "generate_paraphrase" or args.prompt_name == 'generate_sentence')) or args.prompt_name == 'paraphrase-task':
                unshuffled = unshuffled.filter(lambda example: example["label"]==1) 
                prompt = DatasetTemplates(
                    f"{dataset_name}"
                    if dataset_config_name is None
                    else f"{dataset_name}/{dataset_config_name}"
                ) 
            elif (dataset_name == 'duorc' and (args.prompt_name == "generate_question_by_answer" or args.prompt_name == 'build_story_around_qa')):
                unshuffled = unshuffled.filter(lambda example: example["no_answer"]==False)
                prompt = DatasetTemplates(
                    f"{dataset_name}"
                    if dataset_config_name is None
                    else f"{dataset_name}/{dataset_config_name}"
                )    
            else:
                unshuffled = load_dataset(dataset_name, dataset_config_name, ignore_verifications= True)[type_path]    
                prompt = DatasetTemplates(
                    f"{dataset_name}"
                    if dataset_config_name is None
                    else f"{dataset_name}/{dataset_config_name}"
                )                               
            if len(unshuffled)>args.valid_data_size:
                shuffled = unshuffled.shuffle(seed=42)
                unshuffled=shuffled.select(range(args.valid_data_size))
            dataset=unshuffled
            return dataset, prompt, ["","","","",""]
class Pretrain(Dataset):
    def __init__(self, dataset, tokenizer, type_path, input_length, output_length, args):
        self.args = args
        self.tokenizer = tokenizer
        self.type_path = type_path
        self.category = None
        self.whole_dataset = dataset
        self.dataset_name = dataset
        if len(dataset.split('/'))>1:
            self.dataset_name = dataset.split('/')[0]
            self.dataset_config_name = dataset.split('/')[1]
        else:
            self.dataset_name = dataset
            self.dataset_config_name = None
        ids_to_answers = None       
        # dataset not used for training (anli, story cloze, superglue, hellaswag, winogrande, lambada)
        self.dataset, self.prompt, [self.task_prefix, self.input_prefix, self.output_prefix, self.choice_prefix, self.append_choices_to_input] = dataset_prompt_setting(self.type_path, self.dataset_name, self.dataset_config_name, args)
        
        
        print("First elem of self.dataset is ", self.dataset[0])
        print(f'Length of dataset retrieving is.. {len(self.dataset)}')
        self.input_length = input_length
        self.output_length = output_length
        self.ids_to_answers = ids_to_answers


    def __len__(self):
        return len(self.dataset)
    def make_bad_example(self, options, prompt_name, index, answer, example_batch, length =2):
    
        if type(options)==list:
            return options
        else:
            new_option = [answer]
            if self.args.answer_dict_dir_path:
                #print(os.path.join(self.args.answer_dict_dir_path,self.dataset+'.json'))
                with open(os.path.join(self.args.answer_dict_dir_path,self.whole_dataset+'.json')) as f:
                    answer_dict = json.load(f)
            while len(new_option)<length:
                flag = True
                new_option_candidate = random.choice(answer_dict[prompt_name])
                for candidates in new_option:
                    if exact_match_score(new_option_candidate, candidates):
                        flag = False
                if flag == True:
                    new_option.append(new_option_candidate)
            return new_option


    def convert_to_feature_tokenizer(self, input_, target_, options):
        if 'bigscience/T0' in self.args.model_name_or_path: 
            source = self.tokenizer.batch_encode_plus([str(input_)], max_length=self.input_length, 
                                                            padding='max_length', truncation=True, return_tensors="pt", add_special_tokens=False)
        else: 
            source = self.tokenizer.batch_encode_plus([str(input_)], max_length=self.input_length, 
                                                            padding='max_length', truncation=True, return_tensors="pt")
        
        targets = self.tokenizer.batch_encode_plus([str(target_)], max_length=self.output_length, 
                                                    padding='max_length', truncation=True, return_tensors="pt")

        data_label = self.whole_dataset 
        return source, targets, data_label, options

    def convert_to_features_binary(self, example_batch, index):
        name = prompt_choice(self.whole_dataset, self.type_path, self.prompt, example_batch, self.args)
        prompt = self.prompt[name]
        source_ids_batch=[]
        target_ids_batch=[]
        src_mask_batch=[]
        target_mask_batch=[]
        answer = prompt.apply(example_batch)[1]
        old_options = prompt.get_answer_choices_list(example_batch)
        options = self.make_bad_example(old_options, name, index, answer, example_batch, 2)
        good_label = label_(name, options, answer)
        bad_label = random.choice([i for i in range(0,len(options)) if i!=good_label])
        # print("option_idx, label", option_idx, label)
        gold_label = options[good_label]
        black_label = options[bad_label]
        if self.args.channel:
            input, _, instruct = prompt.my_apply_instruct_helper(example_batch)
            instruct = prompt.dict_to_string(instruct)

            if self.args.channel_base:
                input_ = gold_label
                target_ = f'instruction: {instruct} input: {input}'
            else:
                input_ = f'input: {input}\n output: {gold_label}'
                target_= instruct

        else: 
            result = prompt.apply(example_batch)
            input_ = result[0]
            target_ = gold_label

        source, targets, data_label, options= self.convert_to_feature_tokenizer(input_, target_, options)
        source_ids = source["input_ids"].squeeze()
        target_ids = targets["input_ids"].squeeze()
        src_mask    = source["attention_mask"].squeeze()
        target_mask = targets["attention_mask"].squeeze()

        if self.args.channel:
            source_ids_batch.append(source_ids)
            src_mask_batch.append(src_mask)
        else: 
            target_ids_batch.append(target_ids)
            target_mask_batch.append(target_mask)


        if self.args.channel:
            input, _, instruct = prompt.my_apply_instruct_helper(example_batch)
            instruct = prompt.dict_to_string(instruct)
            if self.args.channel_base:
                input_ = black_label
                target_ = f'instruction: {instruct} input: {input}'
            else:
                input_ = f'input: {input}\n output: {black_label}'
                target_= instruct
        else: 
            result = prompt.apply(example_batch)
            input_ = result[0]
            target_ = black_label
        source, targets, data_label, options= self.convert_to_feature_tokenizer(input_, target_, options)
        source_ids = source["input_ids"].squeeze()
        target_ids = targets["input_ids"].squeeze()
        src_mask    = source["attention_mask"].squeeze()
        target_mask = targets["attention_mask"].squeeze()

        if self.args.channel:
            source_ids_batch.append(source_ids)
            src_mask_batch.append(src_mask)
            target_ids_batch = target_ids 
            target_mask_batch = target_mask
        else: 
            target_ids_batch.append(target_ids)
            target_mask_batch.append(target_mask)
            source_ids_batch = source_ids 
            src_mask_batch = src_mask
        return source_ids_batch ,src_mask_batch, target_ids_batch, target_mask_batch, data_label, options, good_label

    def bigbench_input(self, input, instruct, options):
        def choices_string(choices, options, append_choices_to_input):
            if append_choices_to_input:
                choices_string = choices+choices.join(options)
            #choices_string = f'{choices}'+f'{choices}'.join(options)
            else:
                choices_string=""
            return choices_string
        input_ = f'{self.task_prefix}{self.input_prefix}{input}{choices_string(self.choice_prefix, options, self.append_choices_to_input)}{self.output_prefix}'           
        return input_

    def convert_to_features(self, example_batch, index):
        # prompt evaluation
        options = 0
        label = None
        if self.type_path =='bigbench':
            instruct, input, target_, options, label = bigbench_load_example(example_batch, self.prompt)
            #input_= instruct+'\n'+input
            input_ = self.bigbench_input(input, instruct, options)
        elif self.type_path in ['validation', 'test']:
            name = prompt_choice(self.whole_dataset, self.type_path, self.prompt, example_batch, self.args)
            prompt = self.prompt[name]
            result = prompt.apply(example_batch)
            input_ = result[0]
            target_= result[1]
            if self.args.mode == 'zerotune':
                options = None
            options = prompt.get_answer_choices_list(example_batch)

            label = label_(name, options, prompt.apply(example_batch)[1])
        elif self.type_path == 'train':
            name = prompt_choice(self.whole_dataset, self.type_path, self.prompt, example_batch, self.args)
            prompt = self.prompt[name]
            result = prompt.apply(example_batch)
            # print('#######################')
            # print(self.prompt)
            # print()
            # print(name)
            # print()
            # print(prompt)
            # print()
            # print(example_batch)
            # print()
            # print(result)
            # print('#######################')
            if self.args.channel_base:
                target_ = result[0]
                input_= result[1]
            elif self.args.channel:
                input, _, instruct = prompt.my_apply_instruct_helper(example_batch)
                new_instruct = prompt.dict_to_string(instruct)
                input_ = f'input: {input}\n output: {result[1]}'
                target_= new_instruct
            else:
                input_ = result[0]
                target_= result[1]
            if self.args.mode == 'zerotune':
                options = None
            else:
                options = prompt.get_answer_choices_list(example_batch)

            label = -1
        if self.args.label_generalization!='':
            options, target_, input_ = var_direct(self.args.label_generalization,options, target_, input_)
        #print("input:\n", input_)
        #print("target:",target_)
        #print("option and answer is", options, options[label])
        source, targets, data_label, options = self.convert_to_feature_tokenizer(input_, target_, options)
        return source, targets, data_label, options, label


    # channel validation
    def convert_to_features_multiple(self, example_batch, index):

        source_ids_batch=[]
        target_ids_batch=[]
        src_mask_batch=[]
        target_mask_batch=[]

        if self.type_path =='bigbench':
            bigbench_instruct, bigbench_input, _, options, label = bigbench_load_example(example_batch, self.prompt)
            for target in options:
                if self.args.channel_base:
                    input_ = target
                    target_= self.bigbench_input(bigbench_input, bigbench_instruct, options)
                    new_options = options
                else: 
                    # f'{self.task_prefix}{self.input_prefix}{input}{choices_string(self.choice_prefix, options, self.append_choices_to_input)}{self.output_prefix}'
                    instruct = {}
                    input = ""
                    idx=0
                    if self.task_prefix!="":
                        instruct[f'<extra_id_{idx}>']=self.task_prefix
                        input+=f'<extra_id_{idx}>'
                        idx+=1
                    if self.input_prefix!="":
                        instruct[f'<extra_id_{idx}>']=self.input_prefix
                        input+=f'<extra_id_{idx}>'  
                        idx+=1 
                    input+=bigbench_input     
                    if self.append_choices_to_input==True:
                        instruct[f'<extra_id_{idx}>']=self.choice_prefix
                        for option in options:
                            input+= f'<extra_id_{idx}>{option}' 
                        idx+=1
                    if self.output_prefix!="":
                        instruct[f'<extra_id_{idx}>']=self.output_prefix
                        input+=f'<extra_id_{idx}>'  
                        idx+=1                                              
                    if self.args.label_generalization != '':
                        new_options, new_target, new_instruct = var(self.args.label_generalization ,options, target, instruct)
                    else:
                        new_options = options
                        new_target = target
                        new_instruct = instruct
                    target_ = ""
                    for key, value in new_instruct.items():
                        target_+=f"{key} {value}"
                    input_ = f'input: {input}\n output: {new_target}'
                print("channel input:\n", input_)
                print("target:\n", target_)
                #print("option and answer is", new_options, new_options[label])
                source, targets, data_label, new_options= self.convert_to_feature_tokenizer(input_, target_, new_options)
                source_ids = source["input_ids"].squeeze()
                target_ids = targets["input_ids"].squeeze()
                src_mask    = source["attention_mask"].squeeze()
                target_mask = targets["attention_mask"].squeeze()
                source_ids_batch.append(source_ids)
                target_ids_batch.append(target_ids)
                src_mask_batch.append(src_mask)
                target_mask_batch.append(target_mask)
        else:
            name = prompt_choice(self.whole_dataset, self.type_path, self.prompt, example_batch, self.args)
            prompt = self.prompt[name]
            options = prompt.get_answer_choices_list(example_batch)
            label = label_(name, options, prompt.apply(example_batch)[1])
            for target in prompt.get_answer_choices_list(example_batch):
                if self.args.channel_base:
                    result = prompt.apply(example_batch)
                    input_ = target
                    target_= result[0]
                    new_options = options
                    if self.args.label_generalization != '':
                        new_options, input_, target_ = var_direct(self.args.label_generalization,options, input_, target_)


                else: 
                    input, _, instruct = prompt.my_apply_instruct_helper(example_batch)
                    if self.args.label_generalization != '':
                        #print(options)
                        new_options, new_target, new_instruct = var(self.args.label_generalization ,options, target, instruct)
                    else:
                        new_options = options
                        new_target = target
                        new_instruct = instruct
                    new_instruct = prompt.dict_to_string(new_instruct)
                    input_ = f'input: {input}\n output: {new_target}'
                    target_= new_instruct
                print("channel input is", input_)
                print("target is", target_)
                #print("option and answer is", new_options, new_options[label])
                source, targets, data_label, new_options= self.convert_to_feature_tokenizer(input_, target_, new_options)
                source_ids = source["input_ids"].squeeze()
                target_ids = targets["input_ids"].squeeze()
                src_mask    = source["attention_mask"].squeeze()
                target_mask = targets["attention_mask"].squeeze()
                source_ids_batch.append(source_ids)
                target_ids_batch.append(target_ids)
                src_mask_batch.append(src_mask)
                target_mask_batch.append(target_mask)
        return source_ids_batch ,src_mask_batch, target_ids_batch, target_mask_batch, data_label, new_options, label

    def __getitem__(self, index):
        indexed_data = self.dataset[index]    
        #1 only direct (no ul loss case & ul loss validation) & channel base
        if (not self.args.channel and (not self.args.ul_loss or (self.args.ul_loss and self.type_path == 'validation'))) or ((not self.args.ul_loss) and self.type_path == 'train' and self.args.channel):
            source, targets, data_label, options, label = self.convert_to_features(indexed_data, index)
            source_ids = source["input_ids"].squeeze()
            target_ids = targets["input_ids"].squeeze()
            src_mask    = source["attention_mask"].squeeze()
            target_mask = targets["attention_mask"].squeeze()
            if options is not None:
                option_list = options
            else:
                option_list = -1
            return {"source_ids": source_ids, "source_mask": src_mask, "target_ids": target_ids, "target_mask": target_mask, "data_label": data_label, "option_list": option_list, "label": label}
        #2 channel & direct with ul loss training 
        elif (self.args.ul_loss and self.type_path == 'train'):
            source_ids_batch, src_mask_batch, target_ids_batch,  target_mask_batch, data_label, options, label = self.convert_to_features_binary(indexed_data, index)
            if options!=[]:
                option_list = options
            else:
                option_list = -1
            if self.args.channel:
                return {"source_ids":torch.stack(source_ids_batch).reshape(1,-1).squeeze(), "source_mask": torch.stack(src_mask_batch).reshape(1,-1).squeeze(), "target_ids": target_ids_batch, "target_mask": target_mask_batch, "label": label}
            else: 
                return {"source_ids":source_ids_batch, "source_mask": src_mask_batch, "target_ids": torch.stack(target_ids_batch).reshape(1,-1).squeeze(), "target_mask": torch.stack(target_mask_batch).reshape(1,-1).squeeze(), "label": label}
        #3 channel validation - classification
        else:
            
            source_ids_batch, src_mask_batch, target_ids_batch,  target_mask_batch, data_label, options, label = self.convert_to_features_multiple(indexed_data, index)
            if options!=[]:
                option_list = options
            else:
                option_list = -1
            return {"source_ids":torch.stack(source_ids_batch), "source_mask": torch.stack(src_mask_batch), "target_ids": torch.stack(target_ids_batch), "target_mask": torch.stack(target_mask_batch),"data_label": data_label, "option_list": option_list, "label": label}
